<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="A hexo theme">
    <meta name="keyword"  content="saltyfishyjk, yjk, @saltyfishyjk, @yjk, saltyfishyjk&#39;s Blog">
    <link rel="shortcut icon" href="/img/x.ico">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          「CS231n Course 3」神经网络简介 - saltyfishyjk
        
    </title>

    <link rel="canonical" href="https://saltyfishyjk.github.io/2023/05/24/「CS231n-Course-3」神经网络简介/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="saltyfishyjk's Blog" type="application/atom+xml">
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('../../../../img/default.jpg')
                /*post*/
            
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#CS231n" title="CS231n">CS231n</a>
                            
                              <a class="tag" href="/tags/#Deep Learning" title="Deep Learning">Deep Learning</a>
                            
                              <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                            
                        </div>
                        <h1>「CS231n Course 3」神经网络简介</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by saltyfishyjk on
                            2023-05-24
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">3k</span> and
                                Reading Time <span class="post-count">11</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">saltyfishyjk&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                    
                    <li>
                        <a href="https://home.cnblogs.com/u/saltyfishyjk" target="_blank">Chinese Blog</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h1 id="「CS231n-Course-3」神经网络简介"><a href="#「CS231n-Course-3」神经网络简介" class="headerlink" title="「CS231n Course 3」神经网络简介"></a>「CS231n Course 3」神经网络简介</h1><h2 id="Part-0-前言"><a href="#Part-0-前言" class="headerlink" title="Part 0 前言"></a>Part 0 前言</h2><p>参考资料：</p>
<ul>
<li><a href="https://www.bilibili.com/video/BV1Gb4y1X7Q5?p=12&amp;vd_source=b0a8793490273b09808dbd6f72d3465c">b站 CS231n 4.1 反向传播</a></li>
<li><a href="https://www.bilibili.com/video/BV1Gb4y1X7Q5/?p=13&amp;spm_id_from=pageDriver&amp;vd_source=b0a8793490273b09808dbd6f72d3465c">b站 CS231n 4.2 神经网络</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit">知乎 反向传播笔记</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit">神经网络笔记1（上）</a></li>
<li><a href="https://cs231n.github.io/optimization-2/">CS231n Backprop Note(official)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit">神经网络笔记1（下）</a></li>
<li><a href="https://cs231n.github.io/neural-networks-1/">CS231n Neural Network Note-1(official)</a></li>
</ul>
<h2 id="Part-1-反向传播"><a href="#Part-1-反向传播" class="headerlink" title="Part 1 反向传播"></a>Part 1 反向传播</h2><p>反向传播，<strong>Back Propagation(BP)</strong>，是利用<strong>链式法则（chain rule）</strong>递归计算表达式的梯度的方法。</p>
<h3 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h3><p>对给定函数$f(x)$，其中$x$是输入数据的向量，需要计算函数$f$关于$x$的梯度$\nabla f(x)$。</p>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>在神经网络中，$f$ 对应的是损失函数 $L$，输入 $x$ 中包含训练数据和神经网络的权重。</p>
<p>例子：对于 SVM，损失函数是 SVM 的损失函数，输入包含了训练数据 $(x_i, y_i), i=1 … N$，权重 $W$，偏差 $b$。训练集是给定的，权重是可以控制（更新）的变量。因此，在实践上，尽管可以计算输入数据 $x_i$ 上的梯度，但是为了进行参数更新，通常只计算参数（如 $W, b$ ）的梯度。而 $x_i$ 的梯度在一些情形如可视化以直观理解时可以用上。</p>
<h3 id="从（标量）简单表达式引入"><a href="#从（标量）简单表达式引入" class="headerlink" title="从（标量）简单表达式引入"></a>从（标量）简单表达式引入</h3><p>考虑一个简单的二元乘法函数：</p>
<script type="math/tex; mode=display">
f(x, y) = xy</script><p>对$x$和$y$分别求偏导数：</p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial x} = y \\
\frac{\partial f}{\partial y} = x</script><p>（偏）导数的意义是，当函数变量在某个点周围的极小区域内变化，而导数是变量变化导致的函数在该方向上的变化率，数学定义如下：</p>
<script type="math/tex; mode=display">
\frac{\partial f(x)}{\partial x} = \lim_{h \rightarrow 0}\frac{f(x+h) - f(x)}{h}</script><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>梯度$\nabla f$是偏导数的向量，因此（继续上面的例子）有：</p>
<script type="math/tex; mode=display">
\nabla f = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}] = [y, x]</script><p>特别地，即使梯度实际上是一个向量，但通常仍会使用类似”$x$上的梯度“的（非正式）表述，而非”$x$的偏导数“的正确说法，这是因为前者更简单。</p>
<h4 id="对加法求偏导"><a href="#对加法求偏导" class="headerlink" title="对加法求偏导"></a>对加法求偏导</h4><script type="math/tex; mode=display">
f(x, y)=x+y \rightarrow \frac{\partial f}{\partial x} = 1 \quad \frac{\partial f}{\partial y} = 1</script><h4 id="对-max-求导"><a href="#对-max-求导" class="headerlink" title="对$max$求导"></a>对$max$求导</h4><script type="math/tex; mode=display">
f(x, y) = max(x, y) \rightarrow \frac{\partial f}{\partial x} = 1 (x \ge y) \quad \frac{\partial f}{\partial y} = 1 (y \ge x)</script><p>该式说明如果该变量比另一个大，则梯度是 $1$，反之为 $0$。</p>
<h3 id="使用链式法则计算复合表达式"><a href="#使用链式法则计算复合表达式" class="headerlink" title="使用链式法则计算复合表达式"></a>使用链式法则计算复合表达式</h3><p>对于复杂的包含多个函数的复合函数，如：</p>
<script type="math/tex; mode=display">
f(x, y, z) = (x+y)z</script><p>可以将其拆分为两个部分：</p>
<script type="math/tex; mode=display">
q = x + y \quad f = qz</script><p>对其分别求偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial q} = z \quad \frac{\partial f}{\partial z} = q \\
\frac{\partial q}{\partial x} = 1 \quad \frac{\partial q}{\partial y} = 1</script><p>值得注意的是，$q$ 是我们构造的中间量，我们实际上不关心 $q$（即，不关心 $\frac{\partial f}{\partial q}$），而关心 $f$ 关于 $x,y,z$ 的梯度。<strong>链式法则（chain rule）</strong>给出了对于求解 $f$ 关于 $x,y,z$ 梯度表达式的方式：通过相乘。如：</p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}</script><h4 id="计算图例子"><a href="#计算图例子" class="headerlink" title="计算图例子"></a>计算图例子</h4><p><img src="213da7f66594510b45989bd134fc2d8b_720w.jpeg" alt="img"></p>
<ul>
<li><p>前向传播（forward pass）：<strong>绿色</strong>数字是量的值（ $x,y,z$ 的初始值是我们设定的，没有特殊含义），从左到右（前向）进行依次通过算子（节点（node）/门（gate））</p>
</li>
<li><p>反向传播（backword pass）：<strong>红色</strong>数字是 $f$ 相对于当前变量（假定为 $u$ ）的梯度，从右到左（反向/后向）依次通过算子进行链式法则。$f$ 的梯度初始值是 $1$，这是因为 $\frac{\partial f}{\partial f} = 1$</p>
</li>
</ul>
<h4 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h4><p>每个节点（node），也叫做门（gate），具有前向传播和反向传播两种计算：</p>
<ul>
<li>前向传播计算：输入是若干节点的值，输出是经过该节点的运算结果。如上图中$+$门的两个输入是 $-2$ 和 $5$，输出是 $-2+5=3$</li>
<li>反向传播计算：输入是链式法则上一步的梯度，输出是链式法则的下一步梯度。如上图中$\times$门的输入是 $1$，输出有两个分支方向，对于 $q$ 方向，由于 $\times$ 运算的偏导数是对方（如$f(x, y) = xy \quad \frac{\partial f}{\partial x} = y \quad<br>\frac{\partial f}{\partial y} = x$），因此局部梯度为 $z$ 即为 $-4$，因此链式法则的下一步梯度得到 $1 \times (-4) = -4$；对于 $z$ 方向，由于 $\times$ 运算的偏导数是对方，因此局部梯度为 $q$ 即为$3$，因此链式法则的下一步梯度得到 $1 \times 3 = 3$</li>
</ul>
<p>反向传播可以看作门单元之间在通过梯度信号相互通信。</p>
<h3 id="模块化：以-Sigmoid-为例"><a href="#模块化：以-Sigmoid-为例" class="headerlink" title="模块化：以  $Sigmoid$  为例"></a>模块化：以  $Sigmoid$  为例</h3><p>上面的例子比较直观，我们给出对门更精确的描述：任何可微函数都可以看作门，可以将多个门组合成一个门，也可以根据需求将一个门拆分为多个门。一个复杂的例子：</p>
<script type="math/tex; mode=display">
f(w,x) = \frac{1}{1 + e^{-(w_0x_0+w_1x_1+w_2)}}</script><p>事实上，这个表达式描述了一个含输入$x$和权重$w$的2维神经元，使用了 $Sigmoid$  激活函数。</p>
<p>除了上文的加法门、乘法门和 $max$ 门，还有下述门：</p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{x} \rightarrow \frac{df}{dx}= -\frac{1}{x^2}</script><script type="math/tex; mode=display">
f_c(x)=c+x\rightarrow \frac{df}{dx}=1</script><script type="math/tex; mode=display">
f(x)=e^x \rightarrow \frac{df}{dx}=e^x</script><script type="math/tex; mode=display">
f_a(x)=ax \rightarrow \frac{df}{dx} = a</script><h4 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h4><p><img src="0799b3d6e5e92245ee937db3c26d1b80_720w.webp" alt="img"></p>
<h4 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="$Sigmoid$"></a>$Sigmoid$</h4><p>$Sigmoid$ 函数也被记为 $\sigma (x)$，形式如下：</p>
<script type="math/tex; mode=display">
\sigma (x) = \frac{1}{1 + e^{-x}}</script><p>求导结果为：</p>
<script type="math/tex; mode=display">
\frac{d \sigma (x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2}=(\frac{1+e^{-x}-1}{1+e^{-x}})(\frac{1}{1+e^{-x}})=(1 - \sigma (x))(\sigma(x))</script><p>因此，可以将原式转为：</p>
<script type="math/tex; mode=display">
q = w_0x_0+w_1x_1+w_2 \\
f(q) = \frac{1}{1+e^{-q}} = \sigma (q)</script><p>因此，将原来计算图中的 $*-1,exp,+1,\frac{1}{x}$ 合并为 $\sigma $，一次性算出 $(1-0.73) \times 0.73=0.20$ ，这也说明我们可以根据需求将多个门合并为一个门，也可以将一个门拆分为多个门。</p>
<h3 id="回传流中的模式"><a href="#回传流中的模式" class="headerlink" title="回传流中的模式"></a>回传流中的模式</h3><p>多数时候，反向传播中的梯度可以被直观地解释。</p>
<h4 id="加法门"><a href="#加法门" class="headerlink" title="加法门"></a>加法门</h4><p>将输出的梯度相等地分发给所有输入，与输入值在前向传播时的值无关。</p>
<h4 id="max-门"><a href="#max-门" class="headerlink" title="$max$ 门"></a>$max$ 门</h4><p>$max$ 门将梯度转给其中一个输入，该输入是前向传播中值最大的那个输入。</p>
<h4 id="乘法门"><a href="#乘法门" class="headerlink" title="乘法门"></a>乘法门</h4><p>乘法门的局部梯度就是相互交换的输入值，然后根据链式法则乘以输出值的梯度。</p>
<h3 id="对向量操作计算梯度"><a href="#对向量操作计算梯度" class="headerlink" title="对向量操作计算梯度"></a>对向量操作计算梯度</h3><p>之前引入的内容都考虑的单个变量，但所有概念都适用于矩阵和向量操作。值得注意的是，操作时需要注意维度和转置操作。</p>
<h4 id="矩阵相乘的梯度"><a href="#矩阵相乘的梯度" class="headerlink" title="矩阵相乘的梯度"></a>矩阵相乘的梯度</h4><p>一个有用的技巧：矩阵（向量）的尺寸（维度）和其梯度是一致的，据此我们可以知道如何使维度相符合。比如，$X$的尺寸是$[10, 3]$， $dD$的尺寸是 $[5, 3]$，如果想要 $dW$ 和 $W$ 的尺寸是 $[5, 10]$ ，那么需要 $dD.dot(X.T)$</p>
<h2 id="Part-2-神经网络"><a href="#Part-2-神经网络" class="headerlink" title="Part 2 神经网络"></a>Part 2 神经网络</h2><h3 id="线性分类与神经网络"><a href="#线性分类与神经网络" class="headerlink" title="线性分类与神经网络"></a>线性分类与神经网络</h3><p>在之前的<strong>线性分类</strong>笔记中，我们使用了 $s=Wx$ 来计算不同视觉类别的评分，其中 $W$ 是一个权重矩阵， $x$ 是一个包含了图像的全部像素数据的输入列向量。具体地，在CIFAR-10中， $x$ 是 $[3072 \times 1]$ 的列向量，$W$ 是 $[10 \times 3072]$ 的矩阵，输出的评分是 $[10 \times 1]$ 列向量，表示 $10$ 个分类的评分。</p>
<p>而<strong>神经网络</strong>有所不同，（一个两层神经网络，2-layer Neural Net）的计算公式是：</p>
<script type="math/tex; mode=display">
s=W_2max(0,W_1x)</script><p>其中， $W_1$ 的含义是：如，它可以是一个 $[100\times 3072]$ 的矩阵，可以将图像转为一个 $[100 \times 1]$ 的过渡向量。</p>
<p>函数 $max(0, -)$ 是非线性的，会作用于每个元素，称为<strong>激活函数</strong>，它的选择有很多种。在这里， $max$ 函数简单地设置了阈值，将所有小于 $0$ 的值变为 $0$ 。</p>
<p>矩阵 $W_2$ 的尺寸是 $[10 \times 100]$ ，最终可以得到一个 $[10 \times 1]$ 的列向量，每个元素（数字）可以解释为分类的评分。</p>
<p>在上面，我们注意到神经网络和线性分类的重要区别在于非线性函数。假如没有非线性函数，那么两个矩阵会乘在一起，合并为一个矩阵，那么就会重新变成线性函数（分类）。因此，这个非线性函数就是变化的关键。</p>
<p>参数 $W_1$ 和 $W_2$ 会通过随机梯度下降来学习，他们的梯度在反向传播中通过链式法则求导计算。</p>
<p>类似地，三层神经网络可以看做：</p>
<script type="math/tex; mode=display">
s=W_3 max(0, W_2max(0, W_1x))</script><p>其中， $W_1, W_2, W_3$ 是需要进行学习的参数，中间隐层（hidden layer）的尺寸是网络的<strong>超参数</strong>，后续会学习如何设置。</p>
<h3 id="神经元（Neuron）"><a href="#神经元（Neuron）" class="headerlink" title="神经元（Neuron）"></a>神经元（Neuron）</h3><p>神经网络算法最开始是受到生物神经系统建模的启发，但是很快与其分道扬镳，成为一个工程问题。因此，这方面的讨论局限于高度抽象的简略描述，不做过多的类比牵连。</p>
<h4 id="连接与信号"><a href="#连接与信号" class="headerlink" title="连接与信号"></a>连接与信号</h4><p><img src="neuron.jpg" alt="neuron"></p>
<p>左边是生物神经元，右边是（神经网络中的）神经元计算模型。</p>
<ul>
<li>生物模型中，每个神经元从<strong>树突</strong>获得输入信号，沿着<strong>唯一轴突</strong>产生输出信号。轴突在末端会分支，通过突触和其他神经元的树突相连。</li>
<li>计算模型中，沿着轴突传播的信号（这里是 $x_0$），传播至下一个神经元时，会基于突触的强度（这里是 $w_0$），和该神经元的树突进行乘法交互（这里是 $w_0x_0$）。这里，突触的强度（也就是权重 $w$ ）是可以学习的，而且可以控制一个神经元对另一个神经元影响的强度（还可以控制影响方向：使其兴奋（正权重）或抑制（负权重））</li>
<li>生物模型中，树突将信号传递到细胞体，信号在细胞体中相累加，如果和高于某个阈值，则神经元被<strong>激活</strong>，向其轴突输出一个峰值信号。</li>
<li>计算模型中，假设峰值信号的准确时间点不重要，重要的是激活信号的频率。这个基于<em>速率编码（firing rate）</em>的观点将神经元的激活率建模为<strong>激活函数（activation function）$f$ </strong>，表达轴突上的激活信号的频率。</li>
<li>历史上激活函数常选择使用 sigmoid 函数 $\sigma$，该函数输入实数值（求和后的信号值），将输入值压缩到 0-1 之间。</li>
</ul>
<h3 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h3><h4 id="Sigmoid-1"><a href="#Sigmoid-1" class="headerlink" title="$Sigmoid$"></a>$Sigmoid$</h4><p>$Sigmoid$ 公式如下：</p>
<script type="math/tex; mode=display">
\sigma (x) = \frac{1}{1 + e^{-x}}</script><p>函数图像如下：</p>
<p><img src="sigmoid-function.png" alt="Sigmoid Function: - Artificial Intelligence"></p>
<p>$Sigmoid$ 函数输入实数值，并将其压缩到 $0$ 到 $1$ 的范围内。具体地，很大的负数变为 $0$ ，很大的正数变为 $1$ 。</p>
<p>$Sigmoid$ 对于神经元的激活频率有良好的解释：从完全不激活（$0$）到在求和后的最大频率处的完全饱和的激活（$1$）。尽管历史上比较常用，但是现在已经很少使用了，因为有两个主要缺点：</p>
<ul>
<li>$Sigmoid$ 函数饱和使梯度消失。当神经元的激活在接近 $0$ 或 $1$ 处时会饱和，梯度接近 $0$。由于反向传播时，局部梯度会和输出的梯度相乘，如果局部梯度很小（接近 $0$ ），那么相乘的结果会接近 $0$ ，这回“杀死”梯度。</li>
<li>$Sigmoid$ 函数的输出不是零中心的。</li>
</ul>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="$tanh$"></a>$tanh$</h4><p>$tanh$ 公式如下：</p>
<script type="math/tex; mode=display">
tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}} \\
tanh(x)=2\sigma (2x) - 1</script><p>函数图像如下：</p>
<p><img src="R-C.png" alt="R-C"></p>
<h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p>ReLU公式如下：</p>
<script type="math/tex; mode=display">
f(x) = max(0,x)</script><blockquote>
<p>TODO</p>
</blockquote>
<h4 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h4><p>Leakly ReLU公式如下：</p>
<script type="math/tex; mode=display">
f(x) = \mathbb{l} (x<0)(\alpha x) + \mathbb{l}(x \ge 0)(x)</script><blockquote>
<p>TODO</p>
</blockquote>
<h4 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h4><p>Maxout公式如下：</p>
<script type="math/tex; mode=display">
max(w_{1}^{T}x+b_{1}, w_2^{T}x+b_2)</script><p>是 ReLU 和 Leakly ReLU 的一般化归纳。</p>
<blockquote>
<p>TODO</p>
</blockquote>
<h3 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h3><blockquote>
<p>学习神经网络结构非常重要，可以帮助阅读文献的架构图（笔者注）</p>
</blockquote>
<h4 id="分层组织（layer-wise-organization）"><a href="#分层组织（layer-wise-organization）" class="headerlink" title="分层组织（layer-wise organization）"></a>分层组织（layer-wise organization）</h4><p><strong>将神经网络算法以神经元的形式图形化</strong>。神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。这也就是说，一些神经元的输出是另一些神经元的输入。神经网络中不允许循环，因为循环会导致前向传播（正向传播，forward pass）的无限循环。</p>
<p>通常，神经网络模型中的神经元是分层的，这不像生物神经元会聚合成大小不一的团状。对于普通的神经网络，最普通的层的类型是<strong>全连接层（fully-connected layer）</strong>。</p>
<p><img src="fully-connected layer.png" alt="fully-connected layer"></p>
<blockquote>
<p>左边是2层神经网络（2-layer neural network），右侧是3层神经网络（3-layer neural network）</p>
</blockquote>
<p>全连接层中的神经元与前后两层的神经元是完全成对连接的，但同一个全连接层内的神经元之间没有连接。</p>
<p>上图中左侧的2层神经网络的隐层（hidden layer）包含4个神经元（neural，也可以称为单元unit），输入层由3个神经元组成，输出层由2个神经元组成；右侧图是一个3层神经网络，有2个包含4个神经元的隐层。在这两个神经网络中，层与层之间的神经元是全连接的，但是层内的神经元不连接。</p>
<h4 id="命名规则"><a href="#命名规则" class="headerlink" title="命名规则"></a>命名规则</h4><p>当说“N层神经网络（N-layer neural network）”时，并<strong>没有将输入层算入</strong>。因此，单层的神经网络是没有隐层的（指输入直接映射到输出）。因此，有人认为逻辑回归（logistic regression）和支持向量机（SVM）是一种单层神经网络（single-layer neural network）的特例。</p>
<p>人们也会使用人工神经网络（Artificial Neural Network, ANN）或多层感知器（Multi-Layer Perceptrons, MLP）来指代神经网络（Neural Network, NN）。</p>
<p>也有人不喜欢神经网络算法和人类大脑的类比，他们更倾向于使用单元（unit）而非神经元（neuron）作为术语。</p>
<h5 id="输出层（Output-layer）"><a href="#输出层（Output-layer）" class="headerlink" title="输出层（Output layer）"></a>输出层（Output layer）</h5><p>和神经网络中的其他层不同，输出层的神经元一般没有激活函数（或者认为有一个线性相等的激活函数）。这是因为最后的输出层大多用来表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归regession中）。</p>
<h5 id="确定网络尺寸（Sizing-neural-networks）"><a href="#确定网络尺寸（Sizing-neural-networks）" class="headerlink" title="确定网络尺寸（Sizing neural networks）"></a>确定网络尺寸（Sizing neural networks）</h5><p>用来度量神经网络尺寸的标准主要有两个：神经元个数和参数个数。以上图两个网络举例：</p>
<ul>
<li><p>左侧网络有 $4+2=6$ 个神经元（输入层不算），$[3 \times 4] + [4 \times 2]=20$ 个 权重（weights），$4+2=6$ 个偏置（bias），共 $26$ 个可学习的参数。</p>
</li>
<li><p>右侧网络有 $4+4+2=10$ 个神经元（输入层不算），$[3 \times 4]+[4 \times 4]+[4 \times 1]=32$ 个权重（weights）， $4+4+1=9$ 个偏置（bias），共 $41$ 个可学习的参数。</p>
</li>
</ul>
<p>常识：现代卷积神经网络能包含约1亿个参数，可由 $10-20$ 层构成（深度学习，deep learning）。然而，有效连接（effective connections）的个数因为参数共享的缘故大大增多，这在后面CNN的章节会详细介绍。</p>
<h4 id="前向传播计算举例（feed-forward-computation）"><a href="#前向传播计算举例（feed-forward-computation）" class="headerlink" title="前向传播计算举例（feed-forward computation）"></a>前向传播计算举例（feed-forward computation）</h4><p><img src="fully-connected layer.png" alt="fully-connected layer"></p>
<p><strong>不断重复的矩阵乘法和激活函数相交织。</strong></p>
<p>将神经网络组织成层状的一个重要原因，就是这个结构让神经网络算法使用矩阵向量操作变得简单和高效。以上图右侧的三层神经网络举例，输入是 $[3 \times 1]$ 的向量。<strong>一个层所有连接的强度（all connections strength）可以被存在一个单独的矩阵中</strong>。如第一个隐层权重 $W_1$ 是 $[4 \times 3]$ 的，所有单元的偏置储存在 $b_1$ 中，尺寸是 $[4 \times 1]$ 。这样，每个神经元的权重保存在 $W_1$ 的一个<strong>行</strong>中（这里笔者有结合神经元结构的思考，详见下面的注释），因此矩阵乘法 np.dot(W1,x) 可以计算该层所有神经元的激活数据。类似地， $W_2$ 是 $[4 \times 4]$ 矩阵，存储第二个隐层的连接强度，$W_3$ 是 $[1 \times 4]$ 用于输出层。因此可以总结，完成的 $3$ 层神经网络的前向传播就是简单的 $3$ 次矩阵乘法，交织着激活函数的应用。</p>
<blockquote>
<p>对于上图右侧的神经网络的第一个隐层（hidden layer 1），该层的每个神经元 $unit<em>{2,i}, i \in {1,2,3,4}$ （$unit</em>{2,i}$ 表示第 $2$ 层第 $i$ 个神经元）会接收来自上层的（这里是input layer）每个神经元传递的信号（如 $unit<em>{1,j}, j \in {1,2,3}$ ），以进行 $unit</em>{2,i}$ 的信号计算（$\sum<em>{j}w_jx_j, j \in {1,2,3}$，其中 $w_j$ 是 $unit</em>{2, i}$ 对上一层第 $j$ 个神经元的权重，$x_j$ 是上一层第 $j$ 个神经元传播出的信号）。因此，将 $w_j, j \in {1,2,3}$ 写成一个 $[3 \times 1]$ 的向量即为上文中描述的“ $W_1$ 中的一个行”。</p>
<p>因此， $W_1$ 的尺寸为 $[4 \times 3]$ 中，$4$ 指本层的神经元个数，$3$ 指上层的神经元个数。因此容易知道 $W_2$ 的尺寸是 $[4 \times 4]$</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个3层神经网络的前向传播:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: <span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-x)) <span class="comment"># 激活函数(用的sigmoid)</span></span><br><span class="line">x = np.random.randn(<span class="number">3</span>, <span class="number">1</span>) <span class="comment"># 含3个数字的随机输入向量(3x1)</span></span><br><span class="line">h1 = f(np.dot(W1, x) + b1) <span class="comment"># 计算第一个隐层的激活数据(4x1)</span></span><br><span class="line">h2 = f(np.dot(W2, h1) + b2) <span class="comment"># 计算第二个隐层的激活数据(4x1)</span></span><br><span class="line">out = np.dot(W3, h2) + b3 <span class="comment"># 神经元输出(1x1)</span></span><br></pre></td></tr></table></figure>
<p>在上述代码中 $W_1, W_2, W_3, b_1, b_2, b_3$ 都是网络中可学习的参数。值得注意的是，$x$ 并不是一个单独的列向量，而可以是一个批量训练数据（其中每个输入样本会是 $x$ 中的一列），所有样本会被并行化地高效计算出来。</p>
<blockquote>
<p>全连接层的前向（正向）传播一般就是先进行一个矩阵乘法，然后加上偏置并使用激活函数。</p>
</blockquote>
<h4 id="表达能力"><a href="#表达能力" class="headerlink" title="表达能力"></a>表达能力</h4><p>理解全连接层神经网络的一个方式是，可以认为其定义了一个由一系列函数组成的函数族（family of functions），网络的权重是每个函数的参数。因此问题是：该函数族的表达能力如何？存在不能被神经网络表达的函数吗？</p>
<p>可以证明拥有至少一个隐层的神经网络是一个通用的近似器。对给定的任意连续函数 $f(x)$ 和任意 $\epsilon &gt; 0$，均存在至少含一个隐层的神经网络 $g(x)$ （且网络中有合理选择的非线性激活函数，如 sigmoid ），对于 $ \forall x$ ，使得 $|f(x) - g(x)|&lt;\epsilon$ 。换句话说，该神经网络可以近似任何连续函数。</p>
<p>既然一个隐层就能近似任何函数，那为什么还要构建更多层来将网络做得更深？答案是：虽然一个2层网络在数学理论上能完美地近似所有连续函数，但在实际操作中效果相对较差。在一个维度上，虽然以 $a,b,c$ 为参数向量“指示块之和”函数 $g(x)=\sum_{i}c_i \mathbb{l} (a_i &lt; x &lt; b_i)$ 也是通用的近似器，但是谁也不会建议在机器学习中使用这个函数公式。神经网络在实践中非常好用，是因为它们表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，网络通过最优化算法（例如梯度下降）能比较容易地学习到这个函数。类似的，虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好。</p>
<p>另外，在实践中3层的神经网络会比2层的表现好，然而继续加深（做到4，5，6层）很少有太大帮助。卷积神经网络的情况却不同，在卷积神经网络中，对于一个良好的识别系统来说，深度是一个极端重要的因素（比如数十(以10为量级)个可学习的层）。对于该现象的一种解释观点是：因为图像拥有层次化结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据就有直观意义。</p>
<h4 id="层的数量和尺寸"><a href="#层的数量和尺寸" class="headerlink" title="层的数量和尺寸"></a>层的数量和尺寸</h4><p>在面对一个具体问题的时候该确定网络结构呢？到底是不用隐层呢？还是一个隐层？两个隐层或更多？每个层的尺寸该多大？</p>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2023/05/24/Marp笔记/" data-toggle="tooltip" data-placement="top" title="Marp笔记">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2023/05/23/「CS231n-Course-2」损失函数/" data-toggle="tooltip" data-placement="top" title="「CS231n Course 2」损失函数">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        This is copyright.
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                <!--  css & js -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <hr>
                <div id="blog_comments"></div>

<!--
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
-->
<!--
<link rel="stylesheet" href="/css/gitment.css">
<script type="text/javascript" src="/js/gitment.js"></script>
-->

<script src="https://cdn.jsdelivr.net/gh/theme-next/theme-next-gitment@1/gitment.browser.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/theme-next/theme-next-gitment@1/default.css"/>

<!--
<link rel="stylesheet" href="https://billts.site/extra_css/gitment.css">
<script src="https://billts.site/js/gitment.js"></script>
-->

<script>
const myTheme = {
  render(state, instance) {
    const container = document.createElement('div')
    container.lang = "en-US"
    container.className = 'gitment-container gitment-root-container'
    
     // your custom component
    container.appendChild(instance.renderSomething(state, instance))
    
    container.appendChild(instance.renderHeader(state, instance))
    container.appendChild(instance.renderComments(state, instance))
    container.appendChild(instance.renderEditor(state, instance))
    //container.appendChild(instance.renderFooter(state, instance))
    return container
  },
  renderSomething(state, instance) {
    const container = document.createElement('div')
    container.lang = "en-US"
    container.className = 'hello_visitor'
    if (state.user.login) {
      container.innerText = `Hello ${state.user.login}, Welcome to comment system`
    }
    return container
  }
}


const gitment = new Gitment({
    id: 'Wed May 24 2023 22:04:48 GMT+0800', // optional
    owner: "saltyfishyjk",
    repo: "saltyfishyjk.github.io",
    oauth: {
      client_id: "17737ec06ef80355865e",
      client_secret: "d79eba1e32e61803f07e93646964a7978e09f515",
    },
    theme: myTheme,
    // ...
    // For more available options, check out the documentation below
  })
  
  gitment.render('blog_comments')
  // or
  // gitment.render(document.getElementById('comments'))
  // or
  // document.body.appendChild(gitment.render())
</script>
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#%E3%80%8CCS231n-Course-3%E3%80%8D%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">「CS231n Course 3」神经网络简介</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Part-0-%E5%89%8D%E8%A8%80"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text">Part 0 前言</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Part-1-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-nav-number">1.2.</span> <span class="toc-nav-text">Part 1 反向传播</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98"><span class="toc-nav-number">1.2.1.</span> <span class="toc-nav-text">核心问题</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%9B%AE%E7%9A%84"><span class="toc-nav-number">1.2.2.</span> <span class="toc-nav-text">目的</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%BB%8E%EF%BC%88%E6%A0%87%E9%87%8F%EF%BC%89%E7%AE%80%E5%8D%95%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BC%95%E5%85%A5"><span class="toc-nav-number">1.2.3.</span> <span class="toc-nav-text">从（标量）简单表达式引入</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-nav-number">1.2.4.</span> <span class="toc-nav-text">梯度</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E5%AF%B9%E5%8A%A0%E6%B3%95%E6%B1%82%E5%81%8F%E5%AF%BC"><span class="toc-nav-number">1.2.4.1.</span> <span class="toc-nav-text">对加法求偏导</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E5%AF%B9-max-%E6%B1%82%E5%AF%BC"><span class="toc-nav-number">1.2.4.2.</span> <span class="toc-nav-text">对$max$求导</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%BD%BF%E7%94%A8%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99%E8%AE%A1%E7%AE%97%E5%A4%8D%E5%90%88%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="toc-nav-number">1.2.5.</span> <span class="toc-nav-text">使用链式法则计算复合表达式</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%BE%8B%E5%AD%90"><span class="toc-nav-number">1.2.5.1.</span> <span class="toc-nav-text">计算图例子</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-nav-number">1.2.5.2.</span> <span class="toc-nav-text">直观理解</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%A8%A1%E5%9D%97%E5%8C%96%EF%BC%9A%E4%BB%A5-Sigmoid-%E4%B8%BA%E4%BE%8B"><span class="toc-nav-number">1.2.6.</span> <span class="toc-nav-text">模块化：以  $Sigmoid$  为例</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-nav-number">1.2.6.1.</span> <span class="toc-nav-text">计算图</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Sigmoid"><span class="toc-nav-number">1.2.6.2.</span> <span class="toc-nav-text">$Sigmoid$</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%9B%9E%E4%BC%A0%E6%B5%81%E4%B8%AD%E7%9A%84%E6%A8%A1%E5%BC%8F"><span class="toc-nav-number">1.2.7.</span> <span class="toc-nav-text">回传流中的模式</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E5%8A%A0%E6%B3%95%E9%97%A8"><span class="toc-nav-number">1.2.7.1.</span> <span class="toc-nav-text">加法门</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#max-%E9%97%A8"><span class="toc-nav-number">1.2.7.2.</span> <span class="toc-nav-text">$max$ 门</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E4%B9%98%E6%B3%95%E9%97%A8"><span class="toc-nav-number">1.2.7.3.</span> <span class="toc-nav-text">乘法门</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%AF%B9%E5%90%91%E9%87%8F%E6%93%8D%E4%BD%9C%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="toc-nav-number">1.2.8.</span> <span class="toc-nav-text">对向量操作计算梯度</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="toc-nav-number">1.2.8.1.</span> <span class="toc-nav-text">矩阵相乘的梯度</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Part-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-nav-number">1.3.</span> <span class="toc-nav-text">Part 2 神经网络</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-nav-number">1.3.1.</span> <span class="toc-nav-text">线性分类与神经网络</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%EF%BC%88Neuron%EF%BC%89"><span class="toc-nav-number">1.3.2.</span> <span class="toc-nav-text">神经元（Neuron）</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E8%BF%9E%E6%8E%A5%E4%B8%8E%E4%BF%A1%E5%8F%B7"><span class="toc-nav-number">1.3.2.1.</span> <span class="toc-nav-text">连接与信号</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-nav-number">1.3.3.</span> <span class="toc-nav-text">常用激活函数</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Sigmoid-1"><span class="toc-nav-number">1.3.3.1.</span> <span class="toc-nav-text">$Sigmoid$</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#tanh"><span class="toc-nav-number">1.3.3.2.</span> <span class="toc-nav-text">$tanh$</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#ReLU"><span class="toc-nav-number">1.3.3.3.</span> <span class="toc-nav-text">ReLU</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Leaky-ReLU"><span class="toc-nav-number">1.3.3.4.</span> <span class="toc-nav-text">Leaky ReLU</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#Maxout"><span class="toc-nav-number">1.3.3.5.</span> <span class="toc-nav-text">Maxout</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-nav-number">1.3.4.</span> <span class="toc-nav-text">神经网络结构</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E5%88%86%E5%B1%82%E7%BB%84%E7%BB%87%EF%BC%88layer-wise-organization%EF%BC%89"><span class="toc-nav-number">1.3.4.1.</span> <span class="toc-nav-text">分层组织（layer-wise organization）</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E5%91%BD%E5%90%8D%E8%A7%84%E5%88%99"><span class="toc-nav-number">1.3.4.2.</span> <span class="toc-nav-text">命名规则</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%EF%BC%88Output-layer%EF%BC%89"><span class="toc-nav-number">1.3.4.2.1.</span> <span class="toc-nav-text">输出层（Output layer）</span></a></li><li class="toc-nav-item toc-nav-level-5"><a class="toc-nav-link" href="#%E7%A1%AE%E5%AE%9A%E7%BD%91%E7%BB%9C%E5%B0%BA%E5%AF%B8%EF%BC%88Sizing-neural-networks%EF%BC%89"><span class="toc-nav-number">1.3.4.2.2.</span> <span class="toc-nav-text">确定网络尺寸（Sizing neural networks）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E4%B8%BE%E4%BE%8B%EF%BC%88feed-forward-computation%EF%BC%89"><span class="toc-nav-number">1.3.4.3.</span> <span class="toc-nav-text">前向传播计算举例（feed-forward computation）</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B"><span class="toc-nav-number">1.3.4.4.</span> <span class="toc-nav-text">表达能力</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#%E5%B1%82%E7%9A%84%E6%95%B0%E9%87%8F%E5%92%8C%E5%B0%BA%E5%AF%B8"><span class="toc-nav-number">1.3.4.5.</span> <span class="toc-nav-text">层的数量和尺寸</span></a></li></ol></li></ol></li></ol></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#CS231n" title="CS231n">CS231n</a>
                        
                          <a class="tag" href="/tags/#Deep Learning" title="Deep Learning">Deep Learning</a>
                        
                          <a class="tag" href="/tags/#Machine Learning" title="Machine Learning">Machine Learning</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://thysrael.github.io/" target="_blank">Thysrael</a></li>
                    
                        <li><a href="https://wxx0105.cn/" target="_blank">贤弟安全</a></li>
                    
                        <li><a href="https://iszry.github.io" target="_blank">Matcha Flavor</a></li>
                    
                        <li><a href="https://master-tan.github.io/" target="_blank">Tan&#39;s Blog</a></li>
                    
                        <li><a href="https://mksasx.github.io/" target="_blank">AustinMa</a></li>
                    
                        <li><a href="https://bluebean-cloud.github.io/" target="_blank">Bluebean</a></li>
                    
                        <li><a href="https://chlience.cn/" target="_blank">Chlience</a></li>
                    
                        <li><a href="https://sunnyduan-oss.github.io/" target="_blank">甜胖妮</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                
                    <li>
                        <a target="_blank"  href="https://github.com/saltyfishyjk">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                

                
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/people/gu-long-mo-yu">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; saltyfishyjk 2025 
                    <br>
                    Powered by 
                    <a href="https://github.com/saltyfishyjk/saltyfishyjk.github.io">
                        <i>saltyfishyjk</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=saltyfishyjk&repo=saltyfishyjk.github.io&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://saltyfishyjk.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🐟&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
